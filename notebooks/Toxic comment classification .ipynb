{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pre_processing.textProcessing import TextPreProcessor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, auc, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred, print_result=False):\n",
    "    \"\"\"\n",
    "    Given a predicted and true value, get the performance measurement of the classifier\n",
    "    \"\"\"\n",
    "    accr=accuracy_score(y_true, y_pred)\n",
    "    precision=precision_score(y_true,y_pred)\n",
    "    recall=recall_score(y_true,y_pred)\n",
    "    pct_maj = max(np.mean(y_true), 1-np.mean(y_true))\n",
    "    \n",
    "    if print_result:\n",
    "        print(f\"% majority class: {pct_maj}, Accuracy: {accr}, Precision: {precision}, Recall: {recall}\")\n",
    "    return (pct_maj, accr, precision, recall)\n",
    "\n",
    "def undersample_data(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Undersample data\n",
    "    \"\"\"\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X_train_DTM, Y_train.toxic)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def get_model_output(model, X_train, Y_train, X_test, Y_test, model_name='', y_labels=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], undersample=False):\n",
    "    \"\"\"\n",
    "    Automated the training and performance of traditional ML models\n",
    "    \"\"\"\n",
    "    output = pd.DataFrame(columns=['model_name', 'label','undersampled','pct_maj_class','accr','precision','recall'])\n",
    "    for label in y_labels:\n",
    "        # Get the label to use\n",
    "        y_train = Y_train[label]\n",
    "        y_test = Y_test[label]\n",
    "        \n",
    "        if undersample:\n",
    "            X_train, y_train = undersample_data(X_train, y_train)\n",
    "        \n",
    "        # Fit the model\n",
    "        fitted = model.fit(X_train, y_train)\n",
    "        pred = fitted.predict(X_test)\n",
    "        \n",
    "        # Get the results\n",
    "        result = [model_name, label, undersample] + list(evaluate_classifier(y_true=y_test, y_pred=pred))\n",
    "        output.loc[len(output)] = result\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training set: (119678,)\n",
      "Shape of the testing set: (39893,)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# Sample the data\n",
    "# train = train.sample(10000, random_state=1)\n",
    "labels = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "\n",
    "## Traning and testing split\n",
    "random.seed(923)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train.comment_text, labels, test_size = 0.25,random_state = 23)\n",
    "\n",
    "X_train.reset_index(drop = True,inplace = True)\n",
    "X_test.reset_index(drop = True,inplace = True)\n",
    "Y_train.reset_index(drop = True,inplace = True)\n",
    "Y_test.reset_index(drop = True,inplace = True)\n",
    "\n",
    "print(f\"Shape of the training set: {X_train.shape}\")\n",
    "print(f\"Shape of the testing set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()\n",
    "\n",
    "def wordcloud(column, colormap, value = True):\n",
    "    if value: \n",
    "        subset = train[train[column] == value]\n",
    "    else: \n",
    "        subset = train[train[column] == 1]\n",
    "    text = subset.comment_text.values\n",
    "    \n",
    "#     image_path = './input/images/'+image\n",
    "#     my_mask = np.array(Image.open(image_path))\n",
    "#     my_mask = my_mask[:,:,1]\n",
    "    \n",
    "    word = WordCloud(width = 1400, height =800,\n",
    "                    background_color = 'white',\n",
    "                    #mask = my_mask,\n",
    "                    max_words = 3000,\n",
    "                    random_state = 50,\n",
    "                    #scale  = 2 \n",
    "                    ).generate(' '.join(text))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'High frequency words in {column.title()} Comments', fontsize = 20)\n",
    "    plt.imshow(word.recolor(colormap = colormap, random_state = 17))\n",
    "    \n",
    "plt.figure(figsize = (12,12))\n",
    "wordcloud('empty_cat', 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "wordcloud('threat', 'Wistia', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Labels by Label')\n",
    "label_count = train.iloc[:, 2:].sum()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for records that have multiple classifications\n",
    "classified = train.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "mult_class = classified.value_counts()\n",
    "\n",
    "mult_class.plot(x=mult_class, \n",
    "             y=mult_class.values, \n",
    "             kind='bar', \n",
    "             xlabel='Number of Labels', \n",
    "             ylabel='Records')\n",
    "plt.title('Number of Labels per Record')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for records that have no classifications and data imbalance\n",
    "train['empty_cat'] = (classified == 0)\n",
    "train['empty_cat'] = train['empty_cat'].astype(int)\n",
    " \n",
    "cat_tot = train.iloc[:, 2:].sum()\n",
    "\n",
    "cat_tot.plot(x=cat_tot, \n",
    "             y=cat_tot.values, \n",
    "             kind='bar', \n",
    "             xlabel='Label', \n",
    "             ylabel='Records')\n",
    "plt.title('Number of Labels by Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total comments:', len(train))\n",
    "print('Total nontoxic comments:', train['empty_cat'].sum())\n",
    "print('Total toxic comments:', len(train) - train['empty_cat'].sum())\n",
    "print('Total labels:' , train.loc[:, 'toxic':'identity_hate'].sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "In this section, we will pre-process the data into two kinds of formats:\n",
    "\n",
    "1. Document-Term Matrix\n",
    "2. Padded numeric sequences\n",
    "\n",
    "We will use the DTM for non RNN models, and the padded numeric sequences for recurrent neural net work based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the documents into DTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoma/envs/ds207/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Define a function that take in a text and process the doc\n",
    "    \"\"\"\n",
    "    return TextPreProcessor(text=text, lemma_flag=True, stem_flag=False).process()\n",
    "\n",
    "# Fit a tf-idf vectorizer and filter-out the terms with less than 15 occurances or appears in more than 90% of the documents \n",
    "vec_tfidf = TfidfVectorizer(ngram_range=(1,1),tokenizer=tokenize,min_df=15, max_df=0.9)\n",
    "vec_tfidf_fitted = vec_tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_DTM = vec_tfidf_fitted.transform(X_train)\n",
    "X_test_DTM = vec_tfidf_fitted.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_DTM.toarray()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the documents into padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "# Create the tokenizer\n",
    "t = Tokenizer()\n",
    "# Fit the tokenizer on the documents\n",
    "t.fit_on_texts(X_train)\n",
    "\n",
    "\"\"\"\n",
    "The word index for keras Tokenizer is ordered based on frequency. Therefore we can do the following according to\n",
    "https://github.com/keras-team/keras/issues/8092\n",
    "\"\"\"\n",
    "t.oov_token = '_unknown_'\n",
    "t.word_index = {e:i for e,i in t.word_index.items() if i <= VOCAB_SIZE} # <= because tokenizer is 1 indexed\n",
    "t.word_index[t.oov_token] = VOCAB_SIZE + 1\n",
    "\n",
    "\"\"\"\n",
    "Apply the tokenizer\n",
    "\"\"\"\n",
    "encoded_docs = t.texts_to_sequences(X_train)\n",
    "\n",
    "\"\"\"\n",
    "Padd the sequences\n",
    "\"\"\"\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(X_train[0])\n",
    "print(\"Vectorized:\")\n",
    "print(encoded_docs[0])\n",
    "print(\"Padded:\")\n",
    "print(padded_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "non_undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Naive Bayes',\n",
    "    undersample=False\n",
    ")\n",
    "\n",
    "undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Naive Bayes',\n",
    "    undersample=True\n",
    ")\n",
    "\n",
    "multi_nb_results = non_undersampled.append(undersampled, ignore_index=True)\n",
    "multi_nb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logicstic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_undersampled = get_model_output(\n",
    "    LogisticRegression(), \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Logistic Regression',\n",
    "    undersample=False\n",
    ")\n",
    "\n",
    "undersampled = get_model_output(\n",
    "    LogisticRegression(), \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Logistic Regression',\n",
    "    undersample=True\n",
    ")\n",
    "\n",
    "logicstic_regression_results = non_undersampled.append(undersampled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logicstic_regression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "non_undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Linear SVC',\n",
    "    undersample=False\n",
    ")\n",
    "\n",
    "undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Linear SVC',\n",
    "    undersample=True\n",
    ")\n",
    "\n",
    "svc_results = non_undersampled.append(undersampled, ignore_index=True)\n",
    "svc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200)\n",
    "non_undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Random Forest (200 trees)',\n",
    "    undersample=False\n",
    ")\n",
    "\n",
    "undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='Random Forest (200 trees)',\n",
    "    undersample=True\n",
    ")\n",
    "\n",
    "rf_results = non_undersampled.append(undersampled, ignore_index=True)\n",
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=200)\n",
    "non_undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='XGBoost (200 trees)',\n",
    "    undersample=False\n",
    ")\n",
    "\n",
    "undersampled = get_model_output(\n",
    "    model, \n",
    "    X_train_DTM, \n",
    "    Y_train, \n",
    "    X_test_DTM, \n",
    "    Y_test, \n",
    "    model_name='XGBoost (200 trees)',\n",
    "    undersample=True\n",
    ")\n",
    "\n",
    "xgb_results = non_undersampled.append(undersampled, ignore_index=True)\n",
    "xgb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 1: Bidirectional LSTM - not undersampled\n",
    "\"\"\"\n",
    "def create_model(vocab_size, num_labels, sequence_length):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(sequence_length,)),\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=sequence_length, mask_zero=True),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Bidirectional(layers.LSTM(64,return_sequences = False,dropout = 0.2,recurrent_dropout = 0.2)),\n",
    "        layers.Dense(25,activation = 'relu'),\n",
    "        layers.Dense(num_labels,activation = 'sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_model(len(t.word_index)+1, num_labels=6, sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "model.compile(\n",
    "    loss=losses.binary_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Define an early-stopping callback\n",
    "cb = [EarlyStopping(monitor='val_loss',patience = 2)]\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "model.fit(padded_docs, Y_train,validation_split=0.2,\\\n",
    "          batch_size=batch_size, epochs=num_epochs,verbose=1,callbacks =cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_doc_test = t.texts_to_sequences(X_test)\n",
    "padded_doc_test = pad_sequences(encoded_doc_test, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "pred = model.predict(padded_doc_test)\n",
    "\n",
    "\"\"\"\n",
    "Construct the final model\n",
    "\"\"\"\n",
    "model_name = 'Bi-LSTM + Embedding'\n",
    "undersampled = False\n",
    "bilstm_results = pd.DataFrame(columns=['model_name', 'label','undersampled','pct_maj_class','accr','precision','recall'])\n",
    "for i in range(6):\n",
    "    y_pred_bin = [x>0.5 for x in pred[:,i]]\n",
    "    label = Y_train.columns[i]\n",
    "    y_test = Y_test\n",
    "    result = [model_name, label, undersampled] + list(evaluate_classifier(y_true=y_test.iloc[:,i], y_pred=y_pred_bin))\n",
    "    bilstm_results.loc[len(bilstm_results)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 2: LSTM only\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([multi_nb_results, svc_results, logicstic_regression_results,rf_results,xgb_results,bilstm_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_csv('../output/all_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds207",
   "language": "python",
   "name": "ds207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
